# basic inforamtion about experiment
config: C:\Users\Machine Learning GPU\Desktop\fyp\fyp\new_method\config.yml
name: deblurring

#update and display frequency
display_step_freq: 1
save_step_freq: 100
eval_step_freq: 10

#about training and testing
evaluate: false
sweep: false
test: true
train: false

# data path
dataset: gopro
data_root_train: C:\Users\Machine Learning GPU\Desktop\GOPRO_Large_all(2)
seq_len_train: [4, 16]

training_augmentations:
  resize: [256, 256]
  # color_jitter: [0.4, 0.4, 0.4, 0.1]
  # random_flip: true
  # random_rotation: 10
  # random_affine: [0, 0, 0, 0.1]
  # random_perspective: [0.1, 0.1, 0.1, 0.1]
  # random_erasing: [0.5, 0.5, 0.5, 0.5, 0.5]

data_root_test: C:\Users\Machine Learning GPU\Desktop\GOPRO_Large_all(2)
seq_len_test: [5, 16]

test_augmentations:
  resize: [256, 256]

# if resume
weights: C:\Users\Machine Learning GPU\Desktop\fyp\fyp\new_method\checkpoints\deblurring_epoch_0_step_900.pth
#  C:\Users\Machine Learning GPU\Desktop\fyp\fyp\new_method\checkpoints\deblurring_initial.pth

checkpoint_dir: C:\Users\Machine Learning GPU\Desktop\fyp\fyp\new_method\checkpoints\
run_dir: C:\Users\Machine Learning GPU\Desktop\fyp\fyp\new_method\runs\
# data to tensorboard
update_sweep_data: false
update_training_loss: true
update_validation_loss: true
update_weights: true

# if visualize
visualize: true
visualization_path: C:\Users\Machine Learning GPU\Desktop\fyp\fyp\new_method\visualization\

#optimizer and optimizer specific parameters
optimizer:
  optimizer_name: AdamW
  weight_decay: 0.0001
  eps: 1e-08

#learning rate scheduler and scheduler specific parameters
if_scheduler: false
scheduler: StepLR

# hyperparameters specification for hyperparameter sweep
num_sweeps: 10
gpus_per_trial: 1

hyperparameters:
  learning_rate:
    type: float
    min: 0.00001
    max: 0.1
  batch_size:
    type: int
    min: 2
    max: 8
    step: 2
  num_epochs:
    type: int
    min: 10
    max: 100
    step: 10
  dropout:
    type: float
    min: 0.1
    max: 0.9
  iterations:
    type: int
    min: 1
    max: 10
    step: 1
  flow_weighting_factor_gamma:
    type: float
    min: 0.1
    max: 0.9
    step: 0.1

# hyperparameters for training
training_parameters:
  lr: 0.005
  batch_size: 6
  num_epochs: 10
  prob_for_frame_drop: 0.2

# hyperparameters for testing
testing_parameters:
  batch_size: 1

num_workers: 4

# model parameters
variational_gen:
  encoder:
    output_channels: 128

  positional:
    output_channels: 16

  latent:
    num_layers: 16
    output_channels: 256
    hidden_size: 256
